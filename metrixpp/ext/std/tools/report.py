#
#    Metrix++, Copyright 2009-2024, Metrix++ Project
#    Link: https://github.com/metrixplusplus/metrixplusplus
#
#    This file is a part of Metrix++ Tool.
#

import logging
import io
import os
import json

from metrixpp.mpp import api
from metrixpp.mpp import utils

DIGIT_COUNT = 8

class Plugin(api.Plugin, api.IConfigurable, api.IRunable):

    def declare_configuration(self, parser):
        self.parser = parser
        parser.add_option("--output-dir", "--od", help="Set the output folder.")
        parser.add_option("--format", "--ft", default='txt', choices=['txt', 'doxygen', 'json'],
                          help="Format of the output data. "
                          "Possible values are 'txt', 'doxygen' or 'json' [default: %default]")

    def configure(self, options):
        self.out_dir = options.__dict__['output_dir']
        self.out_format = options.__dict__['format']

    def initialize(self):
        super(Plugin, self).initialize()

    def loadSubdirs(self, loader, path, subdirs, subfiles):
        aggregated_data = loader.load_aggregated_data(path)

        if not aggregated_data:
            return subdirs, subfiles

        for subfile in aggregated_data.get_subfiles():
            subfiles.append(aggregated_data.path + "/" + subfile)

        for subdir in aggregated_data.get_subdirs():
            subdir = aggregated_data.path + "/" + subdir
            subdirs.append(subdir)
            # recurse for all subdirs and subfiles
            subdirs, subfiles = self.loadSubdirs(loader, subdir, subdirs, subfiles)
        return subdirs, subfiles

    @staticmethod
    def _get_warning_text(warning, doxygen=False):
        warning_text = "Metric '" + warning.namespace + ":" + warning.field + "'"

        ref = "\\ref " if doxygen else ""

        if warning.region_name and warning.region_name != "__global__":
            warning_text = warning_text + " for region " + ref + warning.region_name
        elif warning.region_name == "__global__":
            warning_text = warning_text + " for region " + warning.region_name
        else:
            warning_text = warning_text + " for the file " + ref + warning.path

        warning_text = warning_text + " exceeds the limit."

        if warning.type == "max":
            warning_comp = ">"
        else:
            warning_comp = "<"
        warning_text = warning_text + " (value: {} {} limit: {})".format(warning.stat_level,
                                                                         warning_comp,
                                                                         warning.stat_limit)
        return warning_text

    @staticmethod
    def _get_warning_dict(warning):
        warning_dict = {}

        warning_dict["metric"] = warning.namespace + ":" + warning.field
        warning_dict["region"] = warning.region_name
        warning_dict["type"] = warning.type
        warning_dict["level"] = warning.stat_level
        warning_dict["limit"] = warning.stat_limit

        return warning_dict

    def _get_txt_warnings(self, warnings):
        warning_text = ""
        for warning in warnings:
            warning_text += self._get_warning_text(warning) + "\n"
        return warning_text

    def create_txt_report(self, paths, overview_data, data):
        report_text = "Overview:\n"

        # start with overview data
        for row in overview_data["matrix"]:
            report_text += "\n"
            for idx, field in enumerate(overview_data["fields"]):
                report_text += field + ": " + str(row[idx]) + "\n"

        if len(overview_data["warnings"]) > 0:
            report_text += "\nWarnings:\n"
            report_text += self._get_txt_warnings(overview_data["warnings"])

        # add file based data
        report_text += "\nFiles:\n"
        for path in paths:
            report_text += "\n" + path + "\n"
            for row in data[path]["file_matrix"]:
                for idx, field in enumerate(data[path]["file_fields"]):
                    report_text += field + ": " + str(row[idx]) + "\n"
            for row in data[path]["region_matrix"]:
                report_text += "\n" + path + " "
                for idx, field in enumerate(data[path]["region_fields"]):
                    report_text += field + ": " + str(row[idx]) + "\n"

            if data[path]["warnings"]:
                report_text += "\nWarnings for " + path + ":\n"
                report_text += self._get_txt_warnings(data[path]["warnings"])

        return report_text

    def create_doxygen_report(self, paths, overview_data, data):

        import pytablewriter

        result_text = ""
        result_text += "/* this file is autogenerated by metrix++ - changes will be overwritten */\n"
        result_text += "/*!\n"

        result_text += "\\page metrix_overview Metrix overview\n\n"

        result_text += "\\section metrix_sec Metrix Warnings\n\n"
        result_text += "Metrix Limits exceeded {} times.\n\n".format(len(overview_data["warnings"]))

        if len(overview_data["warnings"]) > 0:
            result_text += "Warning list: \\ref metrix_warnings\n\n"

        for file_data in overview_data["matrix"]:
            file_data[0] = str(file_data[0]).replace("\\", "/")

        writer = pytablewriter.MarkdownTableWriter()
        writer.table_name = "metrix overview"
        writer.headers = overview_data["fields"]
        writer.value_matrix = overview_data["matrix"]
        writer.margin = 1
        writer.stream = io.StringIO()
        writer.write_table()

        result_text += writer.stream.getvalue() + "\n\n"

        for path in paths:

            result_text += "\\file {}\n\n".format(path)

            writer = pytablewriter.MarkdownTableWriter()
            writer.table_name = "metrix"
            writer.headers = data[path]["file_fields"]
            writer.value_matrix = data[path]["file_matrix"]
            writer.margin = 1
            writer.stream = io.StringIO()
            writer.write_table()

            result_text += writer.stream.getvalue() + "\n"

            for region in data[path]["region_matrix"]:
                if region[0] != "-" and region[0] != "__global__":
                    region[0] = "\\ref " + region[0]

            writer = pytablewriter.MarkdownTableWriter()
            writer.table_name = "region metrix"
            writer.headers = data[path]["region_fields"]
            writer.value_matrix = data[path]["region_matrix"]
            writer.margin = 1
            writer.stream = io.StringIO()
            writer.write_table()

            result_text += writer.stream.getvalue() + "\n"

            # add warnings as list items
            for warning in data[path]["warnings"]:
                warning_text = self._get_warning_text(warning, doxygen=True)
                result_text += "\\xrefitem metrix_warnings \"Metrix Warning\" \"Metrix Warnings\" {}\n".format(warning_text)

            result_text += "\n\n"

        result_text += "*/\n"

        return result_text

    def create_json_report(self, paths, overview_data, data):
        report_dict = {}

        # start with overview data
        overview_list = []
        for row in overview_data["matrix"]:
            overview_dict = {}
            for idx, field in enumerate(overview_data["fields"]):
                overview_dict[field] = str(row[idx])
            overview_list.append(overview_dict)
        report_dict["overview"] = overview_list
        report_dict["warnings"] = []
        for warning in overview_data["warnings"]:
            report_dict["warnings"].append(self._get_warning_dict(warning))

        # add file based data
        files_dict = {}
        for path in paths:
            file_dict = {}
            regions_dict = {}
            warning_list = []

            for row in data[path]["file_matrix"]:
                for idx, field in enumerate(data[path]["file_fields"]):
                    file_dict[field] = str(row[idx])

            for row in data[path]["region_matrix"]:
                region_dict = {}
                for idx, field in enumerate(data[path]["region_fields"]):
                    region_dict[field] = str(row[idx])
                if row[0]:
                    regions_dict[row[0]] = region_dict
                else:
                    regions_dict["__no_region__"] = region_dict

            for warning in data[path]["warnings"]:
                warning_list.append(self._get_warning_dict(warning))

            file_dict["regions"] = regions_dict
            file_dict["warnings"] = warning_list
            files_dict[path] = file_dict

        report_dict["files"] = files_dict

        return json.dumps(report_dict, indent=4)

    def run(self, args):
        exit_code = 0

        data = {}
        overview_data = {}

        loader = self.get_plugin('metrixpp.mpp.dbf').get_loader()
        limit_backend = self.get_plugin('std.tools.limit_backend')

        paths = None
        if len(args) == 0:
            subdirs, paths = self.loadSubdirs(loader, ".", [], [])
        else:
            paths = args

        for path in paths:
            path = utils.preprocess_path(path)
            data[path] = {}
            data[path]["file_data"] = {}
            data[path]["file_fields"] = ["warnings"]
            data[path]["file_matrix"] = [[]]
            data[path]["regions"] = {}
            data[path]["region_fields"] = ["region", "warnings"]
            data[path]["region_matrix"] = []
            data[path]["warnings"] = []

            file_data = loader.load_file_data(path)

            # get warnings from limit plugin
            data[path]["warnings"] = limit_backend.get_all_warnings(path)
            # convert paths to increase readability
            for warning in data[path]["warnings"]:
                warning.path = os.path.relpath(warning.path)

            # load file based data
            data_tree = file_data.get_data_tree()
            for namespace in file_data.iterate_namespaces():
                for field in file_data.iterate_fields(namespace):
                    data[path]["file_data"][namespace + "." +  field[0]] = field[1]
                    data[path]["file_fields"].append(namespace + "." +  field[0])

            for field in data[path]["file_fields"]:
                if field == "warnings":
                    data[path]["file_matrix"][0].append(len(data[path]["warnings"]))
                else:
                    data[path]["file_matrix"][0].append(data[path]["file_data"][field])

            # load region based data
            file_data.load_regions()
            for region in file_data.regions:
                data[path]["regions"][region.name] = {}
                data_tree = region.get_data_tree()
                for namespace in region.iterate_namespaces():
                    for field in region.iterate_fields(namespace):
                        data[path]["regions"][region.name][namespace + "." +  field[0]] = field[1]

                        if not (namespace + "." +  field[0]) in data[path]["region_fields"]:
                            data[path]["region_fields"].append(namespace + "." +  field[0])

            # iterate over all found regions in the file
            for region in data[path]["regions"]:
                # add static columns with region name and warning count
                warning_count = sum(warning.region_name == region for warning in data[path]["warnings"])
                region_row = [region, str(warning_count)]

                # start iterating after the static fields
                for field in data[path]["region_fields"][2:]:
                    if field in data[path]["regions"][region]:
                        region_row.append(data[path]["regions"][region][field])
                    else:
                        region_row.append("-")

                data[path]["region_matrix"].append(region_row)

            # assemble overview table
            overview_data["warnings"] = []
            overview_data["fields"] = ["file", "warnings"]
            overview_data["matrix"] = []
            for key, value in data.items():
                for field in value["file_fields"]:
                    if not field in overview_data["fields"]:
                        overview_data["fields"].append(field)

            for key, value in data.items():
                overview_data["warnings"] = overview_data["warnings"] + value["warnings"]
                row = [os.path.relpath(key), len(value["warnings"])]
                for field in overview_data["fields"][2:]:
                    if field in value["file_data"]:
                        row.append(value["file_data"][field])
                    else:
                        row.append("-")

                overview_data["matrix"].append(row)

        if self.out_format == "txt":
            result_text = self.create_txt_report(paths,
                                                 overview_data,
                                                 data)
            filename = "metrixpp.txt"
        elif self.out_format == "doxygen":
            result_text = self.create_doxygen_report(paths,
                                                     overview_data,
                                                     data)
            filename = "metrixpp.dox"
        elif self.out_format == "json":
            result_text = self.create_json_report(paths,
                                                  overview_data,
                                                  data)
            filename = "metrixpp.json"
        else:
            logging.error("unknown or no output format set")
            return 1

        if self.out_dir:
            os.makedirs(self.out_dir, exist_ok=True)
            with open(os.path.join(self.out_dir, filename), "w+") as result_file:
                result_file.write(result_text)
        else:
            print(result_text)

        return 0
